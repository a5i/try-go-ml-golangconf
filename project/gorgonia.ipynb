{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import (\n",
    "\t\"flag\"\n",
    "\t\"fmt\"\n",
    "\t\"io/ioutil\"\n",
    "\t\"log\"\n",
    "\t\"math/rand\"\n",
    "\t\"os\"\n",
    "\t\"os/signal\"\n",
    "\t\"runtime/pprof\"\n",
    "\t\"syscall\"\n",
    "\n",
    "\t\"github.com/pkg/errors\"\n",
    "\t\"gorgonia.org/gorgonia\"\n",
    "\t\"gorgonia.org/gorgonia/examples/mnist\"\n",
    "\t\"gorgonia.org/tensor\"\n",
    "\n",
    "\t\"time\"\n",
    "\n",
    "\t\"gopkg.in/cheggaaa/pb.v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype := \"float64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const loc = \"../data/\"\n",
    "\n",
    "var dt tensor.Dtype\n",
    "\n",
    "func parseDtype() {\n",
    "\tswitch dtype {\n",
    "\tcase \"float64\":\n",
    "\t\tdt = tensor.Float64\n",
    "\tcase \"float32\":\n",
    "\t\tdt = tensor.Float32\n",
    "\tdefault:\n",
    "\t\tlog.Fatalf(\"Unknown dtype: %v\", dtype)\n",
    "\t}\n",
    "}\n",
    "\n",
    "type sli struct {\n",
    "\tstart, end int\n",
    "}\n",
    "\n",
    "func (s sli) Start() int { return s.start }\n",
    "func (s sli) End() int   { return s.end }\n",
    "func (s sli) Step() int  { return 1 }\n",
    "\n",
    "type convnet struct {\n",
    "\tg                  *gorgonia.ExprGraph\n",
    "\tw0, w1, w2, w3, w4 *gorgonia.Node // weights. the number at the back indicates which layer it's used for\n",
    "\td0, d1, d2, d3     float64        // dropout probabilities\n",
    "\n",
    "\tout *gorgonia.Node\n",
    "}\n",
    "\n",
    "func newConvNet(g *gorgonia.ExprGraph) *convnet {\n",
    "\tw0 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(32, 1, 3, 3), gorgonia.WithName(\"w0\"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n",
    "\tw1 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(64, 32, 3, 3), gorgonia.WithName(\"w1\"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n",
    "\tw2 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(128, 64, 3, 3), gorgonia.WithName(\"w2\"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n",
    "\tw3 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128*3*3, 625), gorgonia.WithName(\"w3\"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n",
    "\tw4 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(625, 10), gorgonia.WithName(\"w4\"), gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n",
    "\treturn &convnet{\n",
    "\t\tg:  g,\n",
    "\t\tw0: w0,\n",
    "\t\tw1: w1,\n",
    "\t\tw2: w2,\n",
    "\t\tw3: w3,\n",
    "\t\tw4: w4,\n",
    "\n",
    "\t\td0: 0.2,\n",
    "\t\td1: 0.2,\n",
    "\t\td2: 0.2,\n",
    "\t\td3: 0.55,\n",
    "\t}\n",
    "}\n",
    "\n",
    "func (m *convnet) learnables() gorgonia.Nodes {\n",
    "\treturn gorgonia.Nodes{m.w0, m.w1, m.w2, m.w3, m.w4}\n",
    "}\n",
    "\n",
    "// This function is particularly verbose for educational reasons. In reality, you'd wrap up the layers within a layer struct type and perform per-layer activations\n",
    "func (m *convnet) fwd(x *gorgonia.Node) (err error) {\n",
    "\tvar c0, c1, c2, fc *gorgonia.Node\n",
    "\tvar a0, a1, a2, a3 *gorgonia.Node\n",
    "\tvar p0, p1, p2 *gorgonia.Node\n",
    "\tvar l0, l1, l2, l3 *gorgonia.Node\n",
    "\n",
    "\t// LAYER 0\n",
    "\t// here we convolve with stride = (1, 1) and padding = (1, 1),\n",
    "\t// which is your bog standard convolution for convnet\n",
    "\tif c0, err = gorgonia.Conv2d(x, m.w0, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 0 Convolution failed\")\n",
    "\t}\n",
    "\tif a0, err = gorgonia.Rectify(c0); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 0 activation failed\")\n",
    "\t}\n",
    "\tif p0, err = gorgonia.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 0 Maxpooling failed\")\n",
    "\t}\n",
    "\tlog.Printf(\"p0 shape %v\", p0.Shape())\n",
    "\tif l0, err = gorgonia.Dropout(p0, m.d0); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Unable to apply a dropout\")\n",
    "\t}\n",
    "\n",
    "\t// Layer 1\n",
    "\tif c1, err = gorgonia.Conv2d(l0, m.w1, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 1 Convolution failed\")\n",
    "\t}\n",
    "\tif a1, err = gorgonia.Rectify(c1); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 1 activation failed\")\n",
    "\t}\n",
    "\tif p1, err = gorgonia.MaxPool2D(a1, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 1 Maxpooling failed\")\n",
    "\t}\n",
    "\tif l1, err = gorgonia.Dropout(p1, m.d1); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Unable to apply a dropout to layer 1\")\n",
    "\t}\n",
    "\n",
    "\t// Layer 2\n",
    "\tif c2, err = gorgonia.Conv2d(l1, m.w2, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 2 Convolution failed\")\n",
    "\t}\n",
    "\tif a2, err = gorgonia.Rectify(c2); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 2 activation failed\")\n",
    "\t}\n",
    "\tif p2, err = gorgonia.MaxPool2D(a2, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Layer 2 Maxpooling failed\")\n",
    "\t}\n",
    "\tlog.Printf(\"p2 shape %v\", p2.Shape())\n",
    "\n",
    "\tvar r2 *gorgonia.Node\n",
    "\tb, c, h, w := p2.Shape()[0], p2.Shape()[1], p2.Shape()[2], p2.Shape()[3]\n",
    "\tif r2, err = gorgonia.Reshape(p2, tensor.Shape{b, c * h * w}); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Unable to reshape layer 2\")\n",
    "\t}\n",
    "\tlog.Printf(\"r2 shape %v\", r2.Shape())\n",
    "\tif l2, err = gorgonia.Dropout(r2, m.d2); err != nil {\n",
    "\t\treturn errors.Wrap(err, \"Unable to apply a dropout on layer 2\")\n",
    "\t}\n",
    "\n",
    "\tioutil.WriteFile(\"tmp.dot\", []byte(m.g.ToDot()), 0644)\n",
    "\n",
    "\t// Layer 3\n",
    "\tif fc, err = gorgonia.Mul(l2, m.w3); err != nil {\n",
    "\t\treturn errors.Wrapf(err, \"Unable to multiply l2 and w3\")\n",
    "\t}\n",
    "\tif a3, err = gorgonia.Rectify(fc); err != nil {\n",
    "\t\treturn errors.Wrapf(err, \"Unable to activate fc\")\n",
    "\t}\n",
    "\tif l3, err = gorgonia.Dropout(a3, m.d3); err != nil {\n",
    "\t\treturn errors.Wrapf(err, \"Unable to apply a dropout on layer 3\")\n",
    "\t}\n",
    "\n",
    "\t// output decode\n",
    "\tvar out *gorgonia.Node\n",
    "\tif out, err = gorgonia.Mul(l3, m.w4); err != nil {\n",
    "\t\treturn errors.Wrapf(err, \"Unable to multiply l3 and w4\")\n",
    "\t}\n",
    "\tm.out, err = gorgonia.SoftMax(out)\n",
    "\treturn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    parseDtype()\n",
    "    rand.Seed(1337)\n",
    "    var inputs, targets tensor.Tensor\n",
    "    var err error\n",
    "    trainOn := \"train\"\n",
    "    if inputs, targets, err = mnist.Load(trainOn, loc, dt); err != nil {\n",
    "       panic(err)\n",
    "    }\n",
    "    \n",
    "    // the data is in (numExamples, 784).\n",
    "    // In order to use a convnet, we need to massage the data\n",
    "    // into this format (batchsize, numberOfChannels, height, width).\n",
    "    //\n",
    "    // This translates into (numExamples, 1, 28, 28).\n",
    "    //\n",
    "    // This is because the convolution operators actually understand height and width.\n",
    "    //\n",
    "    // The 1 indicates that there is only one channel (MNIST data is black and white).\n",
    "    numExamples := inputs.Shape()[0]\n",
    "    bs := 5\n",
    "    // todo - check bs not 0\n",
    "\n",
    "    if err := inputs.Reshape(numExamples, 1, 28, 28); err != nil {\n",
    "        panic(err)\n",
    "    }\n",
    "    g := gorgonia.NewGraph()\n",
    "    x := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(bs, 1, 28, 28), gorgonia.WithName(\"x\"))\n",
    "    y := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(bs, 10), gorgonia.WithName(\"y\"))\n",
    "    m := newConvNet(g)\n",
    "    if err = m.fwd(x); err != nil {\n",
    "        panic(err)\n",
    "    }\n",
    "    losses := gorgonia.Must(gorgonia.HadamardProd(m.out, y))\n",
    "    cost := gorgonia.Must(gorgonia.Mean(losses))\n",
    "    cost = gorgonia.Must(gorgonia.Neg(cost))\n",
    "\n",
    "    // we wanna track costs\n",
    "    var costVal gorgonia.Value\n",
    "    gorgonia.Read(cost, &costVal)\n",
    "\n",
    "    // if _, err = gorgonia.Grad(cost, m.learnables()...); err != nil {\n",
    "    // \tlog.Fatal(err)\n",
    "    // }\n",
    "\n",
    "    // debug\n",
    "    // ioutil.WriteFile(\"fullGraph.dot\", []byte(g.ToDot()), 0644)\n",
    "    // log.Printf(\"%v\", prog)\n",
    "    // logger := log.New(os.Stderr, \"\", 0)\n",
    "    // vm := gorgonia.NewTapeMachine(g, gorgonia.BindDualValues(m.learnables()...), gorgonia.WithLogger(logger), gorgonia.WithWatchlist())\n",
    "\n",
    "    prog, locMap, _ := gorgonia.Compile(g)\n",
    "    log.Printf(\"%v\", prog)\n",
    "\n",
    "    vm := gorgonia.NewTapeMachine(g, gorgonia.WithPrecompiled(prog, locMap), gorgonia.BindDualValues(m.learnables()...))\n",
    "    solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))\n",
    "\n",
    "\n",
    "    batches := numExamples / bs\n",
    "    log.Printf(\"Batches %d\", batches)\n",
    "    bar := pb.New(batches)\n",
    "    bar.SetRefreshRate(time.Second)\n",
    "    bar.SetMaxWidth(80)\n",
    "\n",
    "    for i := 0; i < 10; i++ {\n",
    "        bar.Prefix(fmt.Sprintf(\"Epoch %d\", i))\n",
    "        bar.Set(0)\n",
    "        bar.Start()\n",
    "        for b := 0; b < batches; b++ {\n",
    "            start := b * bs\n",
    "            end := start + bs\n",
    "            if start >= numExamples {\n",
    "                break\n",
    "            }\n",
    "            if end > numExamples {\n",
    "                end = numExamples\n",
    "            }\n",
    "\n",
    "            var xVal, yVal tensor.Tensor\n",
    "            if xVal, err = inputs.Slice(sli{start, end}); err != nil {\n",
    "                panic(\"Unable to slice x\")\n",
    "            }\n",
    "\n",
    "            if yVal, err = targets.Slice(sli{start, end}); err != nil {\n",
    "                panic(\"Unable to slice y\")\n",
    "            }\n",
    "            if err = xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28); err != nil {\n",
    "                log.Panicf(\"Unable to reshape %v\", err)\n",
    "            }\n",
    "\n",
    "            gorgonia.Let(x, xVal)\n",
    "            gorgonia.Let(y, yVal)\n",
    "            if err = vm.RunAll(); err != nil {\n",
    "                log.Panicf(\"Failed at epoch  %d: %v\", i, err)\n",
    "            }\n",
    "            solver.Step(gorgonia.NodesToValueGrads(m.learnables()))\n",
    "            vm.Reset()\n",
    "            bar.Increment()\n",
    "        }\n",
    "        log.Printf(\"Epoch %d | cost %v\", i, costVal)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (lgo)",
   "language": "go",
   "name": "lgo"
  },
  "language_info": {
   "file_extension": "",
   "mimetype": "",
   "name": "go",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
